{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch04. 텍스트 분류\n",
    "## 00. 텍스트 분류란?\n",
    "- 텍스트 분류\n",
    "  - 자연어 처리 기술을 활용해 글의 정보를 추출하여 문제에 맞게 사람이 정한 범주(Class)로 분류하는 문제\n",
    "\n",
    "\n",
    "## 01. 영어 텍스트 분류\n",
    "\n",
    "### 1) 실습 데이터\n",
    "- 캐글 대회의 영화 리뷰 데이터\n",
    "  - Bag of Words Meets Bags of Popcorn (워드팝콘)\n",
    "\n",
    "|||\n",
    "|---|---|\n",
    "|데이터 이름|Bag of Words Meets Bags of Popcorn|\n",
    "|데이터 용도|텍스트 분류 학습 목적|\n",
    "|데이터 권한|MIT 권한을 가지나 캐글 가입 후 사용 권장|\n",
    "|데이터 출처|https://www.kaggle.com/c/word2vec-nip-tutorial/data\n",
    "\n",
    "\n",
    "### 2) 문제 소개\n",
    "- **워드 팝콘**\n",
    "  - 인터넷 영화 데이터베이스(IMDB)에서 나온 영화 평점 데이터를 활용한 캐글 문제\n",
    "  - 영화 평점 데이터이므로 각 데이터는 영화 리뷰 텍스트와 평점에 따른 감정 값(긍정 or 부정)으로 구성\n",
    "  - 보통 감정 분석(sentiment analysis) 문제에서 자주 활용\n",
    "  \n",
    "  \n",
    "- 목표\n",
    "  1. 데이터를 로드 및 전처리\n",
    "  2. 데이터 분석\n",
    "  3. 알고리즘 모델링\n",
    "\n",
    "\n",
    "### 3) 데이터 분석 및 전처리\n",
    "- EDA 과정으로 데이터 전처리 전 데이터를 자세히 탐색해보기\n",
    "\n",
    "#### ① Unzip Word Popcorn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로를 명명해주면 편하게 사용할 수 있음\n",
    "DATA_IN_PATH = './data/word2vec-nlp-tutorial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['labeledTrainData.tsv.zip', 'unlabeledTrainData.tsv.zip', 'testData.tsv.zip']\n",
    "\n",
    "for file in file_list:\n",
    "    zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')\n",
    "    zipRef.extractall(DATA_IN_PATH)\n",
    "    zipRef.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ② import library & data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os # Operating System 제어(e.g 파일이나 폴더 만들기, 복사하기 등)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_IN_PATH+\"labeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 구성\n",
    "  - i, sentiment(긍정 1, 부정 0), review\n",
    "\n",
    "\n",
    "#### ③ 데이터 분석 순서\n",
    "1. 데이터 크기  \n",
    "2. 데이터 개수  \n",
    "3. 각 리뷰 문자 길이 분포  \n",
    "4. 많이 사용된 단어  \n",
    "5. 긍정, 부정 데이터 분포\n",
    "6. 각 리뷰 단어 개수 분포  \n",
    "7. 짝수문자 및 대문자, 소문자 비율  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 크기: \n",
      "labeledTrainData.tsv          33.56MB\n",
      "testData.tsv                  32.72MB\n",
      "unlabeledTrainData.tsv        67.28MB\n",
      "전체 학습데이터 개수: 25000\n"
     ]
    }
   ],
   "source": [
    "## 1. 데이터 크기\n",
    "print(\"파일 크기: \")\n",
    "for file in os.listdir(DATA_IN_PATH):\n",
    "    if 'tsv' in file and 'zip' not in file:\n",
    "        print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB')\n",
    "\n",
    "## 2. 데이터 개수\n",
    "print('전체 학습데이터 개수: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of review')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 3. 각 리뷰 문자 길이 분포\n",
    "train_length = train_data['review'].apply(len)\n",
    "train_length.head()\n",
    "\n",
    "## 위 변수에 각 리뷰의 길이가 담겨 있음\n",
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(12, 5))\n",
    "# 히스토그램 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위\n",
    "# range: x축 값의 범위\n",
    "# alpha: 그래프 색상 투명도\n",
    "# color: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_length, bins=200, alpha=0.5, color= 'r', label='word')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "# 그래프 제목\n",
    "plt.title('Log-Histogram of length of review')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Length of review')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Number of review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰 길이 최댓값: 13710\n",
      "리뷰 길이 최솟값: 54\n",
      "리뷰 길이 평균값: 1329.71\n",
      "리뷰 길이 표준편차: 1005.22\n",
      "리뷰 길이 중간값: 983.0\n",
      "리뷰 길이 제1사분위: 705.0\n",
      "리뷰 길이 제3사분위: 1619.0\n"
     ]
    }
   ],
   "source": [
    "print('리뷰 길이 최댓값: {}'.format(np.max(train_length)))\n",
    "print('리뷰 길이 최솟값: {}'.format(np.min(train_length)))\n",
    "print('리뷰 길이 평균값: {:.2f}'.format(np.mean(train_length)))\n",
    "print('리뷰 길이 표준편차: {:.2f}'.format(np.std(train_length)))\n",
    "print('리뷰 길이 중간값: {}'.format(np.median(train_length)))\n",
    "\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('리뷰 길이 제1사분위: {}'.format(np.percentile(train_length, 25)))\n",
    "print('리뷰 길이 제3사분위: {}'.format(np.percentile(train_length, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x1b902e50610>,\n",
       "  <matplotlib.lines.Line2D at 0x1b902e50970>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x1b902e50cd0>,\n",
       "  <matplotlib.lines.Line2D at 0x1b902e58070>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x1b902e502b0>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x1b902e583d0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x1b902e58a30>],\n",
       " 'means': [<matplotlib.lines.Line2D at 0x1b902e586d0>]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 마크함\n",
    "\n",
    "plt.boxplot(train_length,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. 많이 사용된 단어\n",
    "from wordcloud import WordCloud\n",
    "cloud = WordCloud(width=800, height=600).generate(\" \".join(train_data['review']))\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 워드클라우드 결과 해석\n",
    "  - 데이터에서 가장 많이 사용된 단어: br\n",
    "    - br은 HTML 태그로 해당 데이터셋이 정제되지 않은 인터넷상 리뷰 형태임을 파악할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. 긍정, 부정 데이터 분포\n",
    "fig,axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(6,3)\n",
    "sns.countplot(train_data['sentiment'])\n",
    "\n",
    "print(\"긍정 리뷰 수: {}\".format(train_data['sentiment'].value_counts()[1]))\n",
    "print(\"부정 리뷰 수: {}\".format(train_data['sentiment'].value_counts()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. 각 리뷰 단어 개수 분포\n",
    "train_word_counts = train_data['review'].apply(lambda x:len(x.split(' ')))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(train_word_counts, bins=50, facecolor='r',label='train')\n",
    "plt.title('Log-Histogram of word count in review', fontsize=15)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Number of reviews', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰 단어 개수 최댓값: {}'.format(np.max(train_word_counts)))\n",
    "print('리뷰 단어 개수 최솟값: {}'.format(np.min(train_word_counts)))\n",
    "print('리뷰 단어 개수 평균값: {:.2f}'.format(np.mean(train_word_counts)))\n",
    "print('리뷰 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))\n",
    "print('리뷰 단어 개수 중간값: {}'.format(np.median(train_word_counts)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('리뷰 단어 개수 제1사분위: {}'.format(np.percentile(train_word_counts, 25)))\n",
    "print('리뷰 단어 개수 제3사분위: {}'.format(np.percentile(train_word_counts, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. 특수문자 및 대문자, 소문자 비율\n",
    "# 물음표가 구두점 역할을 함\n",
    "qmarks = np.mean(train_data['review'].apply(lambda x: '?' in x)) \n",
    "# 마침표\n",
    "fullstop = np.mean(train_data['review'].apply(lambda x: '.' in x)) \n",
    "#  첫번째 대문자\n",
    "capital_first = np.mean(train_data['review'].apply(lambda x: x[0].isupper())) \n",
    "# 대문자 개수\n",
    "capitals = np.mean(train_data['review'].apply(lambda x: max([y.isupper() for y in x]))) \n",
    "# 숫자 개수\n",
    "numbers = np.mean(train_data['review'].apply(lambda x: max([y.isdigit() for y in x])))\n",
    "                  \n",
    "print('물음표가있는 질문: {:.2f}%'.format(qmarks * 100))\n",
    "print('마침표가 있는 질문: {:.2f}%'.format(fullstop * 100))\n",
    "print('첫 글자가 대문자 인 질문: {:.2f}%'.format(capital_first * 100))\n",
    "print('대문자가있는 질문: {:.2f}%'.format(capitals * 100))\n",
    "print('숫자가있는 질문: {:.2f}%'.format(numbers * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ④ 데이터 전처리\n",
    "\n",
    "- 사용 라이브러리\n",
    "  - pandas: 데이터 다루기 위한 라이브러리\n",
    "  - numpy: 전처리된 데이터 저장 위한 라이브러리\n",
    "  - re: 정규표현식을 불러오는 라이브러리\n",
    "  - Beautiful Soup: HTML과 XML 파일로부터 데이터를 가져오기 위한 라이브러리\n",
    "  - stopwords: 불용어 제거 함수\n",
    "  - pad_sequences: 텐서플로 전처리 모듈\n",
    "  - Tokenizer: 텐서플로 전처리 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data 리뷰 출력\n",
    "print(train_data['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확인\n",
    "  - HTML 태그와 특수문자 포함  \n",
    "    → Beautiful Soup 이용하여 HTML 태그 제거  \n",
    "    → re.sub 함수로 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = train_data['review'][0]\n",
    "\n",
    "# HTML 태그 제거\n",
    "review_text = BeautifulSoup(review,\"html5lib\").get_text() \n",
    "\n",
    "# 영어 문자를 제외하고 모두 공백 변환\n",
    "review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "\n",
    "print(review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확인\n",
    "  - HTML 태그와 특수문자가 모두 제거된 것 확인\n",
    "\n",
    "- 다음 단계\n",
    "  - 불용어(stopword) 삭제\n",
    "  - 불용어란 문장에서 자주 출현하나 전체 의미에 큰 영향을 주지 않는 단어\n",
    "  - 사용자 정의 가능하나, NLTK 불용어 사전 이용 가능\n",
    "  - NLTK 불용어 사전은 소문자로만 구성  \n",
    "    → 문서를 소문자로 변환한 후 불용어 제거 작업을 해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 영어 불용어 set \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "review_text = review_text.lower()\n",
    "# 단어 기준으로 나누어 단어 리스트 만들기\n",
    "words = review_text.split()\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 리스트\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 단어 리스트를 다시 하나의 글로 합치기\n",
    "clean_review = ' '.join(words)\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 편의를 위해 모든 전처리 과정을 함수로 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing( review, remove_stopwords = False ): \n",
    "    # 불용어 제거는 옵션으로 선택 가능하다.\n",
    "    \n",
    "    # 1. HTML 태그 제거\n",
    "    review_text = BeautifulSoup(review, \"html5lib\").get_text()\t\n",
    "\n",
    "    # 2. 영어가 아닌 특수문자들을 공백(\" \")으로 바꾸기\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "\n",
    "    # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들 나눠서 리스트로 만든다.\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "    if remove_stopwords: \n",
    "        # 4. 불용어들을 제거\n",
    "    \n",
    "        #영어에 관련된 불용어 불러오기\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        # 불용어가 아닌 단어들로 이루어진 새로운 리스트 생성\n",
    "        words = [w for w in words if not w in stops]\n",
    "        # 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.\t\n",
    "        clean_review = ' '.join(words)\n",
    "\n",
    "    else: # 불용어 제거하지 않을 때\n",
    "        clean_review = ' '.join(words)\n",
    "\n",
    "    return clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전체 데이터 전처리 진행\n",
    "clean_train_reviews = []\n",
    "for review in train_data['review']:\n",
    "    clean_train_reviews.append(preprocessing(review, remove_stopwords = True))\n",
    "\n",
    "## 전처리한 데이터 중 하나 출력\n",
    "clean_train_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 지금까지 전처리한 단어를 판다스의 데이터프레임으로 만들어두기\n",
    "## 이유? 모델에 따라 벡터가 아닌 텍스트로만 구성된 값을 사용해야 할 수도 있음\n",
    "clean_train_df = pd.DataFrame({\n",
    "    'review': clean_train_reviews, 'sentiment': train_data['sentiment']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 남은 과정\n",
    "  1. 전처리한 데이터에서 각 단어를 인덱스로 벡터화하기\n",
    "  2. 모델에 따라 입력값 길이가 동일해야 하므로 패딩 과정 진행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer 모듈 생성 후, 인덱스로 구성된 벡터로 변환\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_reviews)\n",
    "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
    "\n",
    "print(text_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확인\n",
    "  - 텍스트로 된 첫 번째 리뷰가 각 단어의 인덱스로 바뀜\n",
    "\n",
    "- 다음 작업\n",
    "  - 각 인덱스가 어떤 단어를 의미하는지 확인 필요\n",
    "    → 단어사전으로 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 단어 사전\n",
    "word_vocab = tokenizer.word_index\n",
    "word_vocab[\"<PAD>\"] = 0\n",
    "print(word_vocab)\n",
    "\n",
    "## 전체 데이터에 사용된 단어 개수 확인\n",
    "print(\"\\n전체 단어 수: \", len(word_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이후 모델에서 사용하기 위해 단어 사전, 전체 단어 개수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_configs = {}\n",
    "\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 패딩 작업\n",
    "  - 각 데이터 길이가 동일해야 모델에 바로 적용할 수 있음\n",
    "    → 특정 길이를 최대 길이로 정함\n",
    "    → 긴 데이터는 뒷부분 자름, 짧은 데이터는 0값으로 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 174 # 최대 길이 지정 시, 보통 중간값을 이용\n",
    "\n",
    "train_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "print('Shape of train data: ', train_inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 시 라벨(정답)을 나타내는 값을 넘파이 배열로 저장\n",
    "  - 이유? 이후에 전처리한 데이터 저장 시 넘파이 형태로 저장하기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_data['sentiment'])\n",
    "print('Shape of label tensor:', train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확인\n",
    "  - 데이터 형태: 데이터 25,000개, 라벨 형태: 길이 25,000  \n",
    "    → 데이터 하나당 하나의 값을 가지는 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⑤ 데이터 저장하기\n",
    "- 텍스트 데이터: CSV 파일\n",
    "- 벡터화한 데이터, 정답 라벨: 넘파이 파일\n",
    "- 데이터 정보(딕셔너리 형태): JSON 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUT_DATA = 'train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "DATA_CONFIGS = 'data_configs.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리한 데이터→ 넘파이 형태로 저장\n",
    "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
    "\n",
    "# 정제된 텍스트→ csv 형태로 저장\n",
    "clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)\n",
    "\n",
    "# 데이터 사전→ json 형태로 저장\n",
    "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평가 데이터 저장\n",
    "  - 라벨 값이 없기 때문에 라벨 저장 필요 없음\n",
    "  - 단어 사전과 단어 개수 정보도 학습 데이터 것을 사용하므로 저장 필요 없음\n",
    "  - **각 리뷰 데이터에 리뷰에 대한 'id'값 지정 필요**\n",
    "  - 나머지는 학습 데이터와 동일하게 전처리 진행\n",
    "\n",
    "- 확인 사항\n",
    "  - 평가 데이터 전처리 시, 토크나이저를 통해 인덱스 벡터로 만들 때, 기존 학습 데이터에 적용한 토크나이저 객체를 사용해야 함\n",
    "  - 새롭게 만드는 경우, 학습 데이터와 평가 데이터에 대한 각 단어 인덱스가 달라져서 모델에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(DATA_IN_PATH + \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test_data['review']:\n",
    "    clean_test_reviews.append(preprocessing(review, remove_stopwords = True))\n",
    "\n",
    "\n",
    "clean_test_df = pd.DataFrame({'review': clean_test_reviews, 'id': test_data['id']})\n",
    "test_id = np.array(test_data['id'])\n",
    "\n",
    "text_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\n",
    "test_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 평가 데이터 저장\n",
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "\n",
    "np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)\n",
    "np.save(open(DATA_IN_PATH + TEST_ID_DATA, 'wb'), test_id)\n",
    "clean_test_df.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 모델링 소개\n",
    "- 전처리된 데이터를 모델에 적용하고, 주어진 텍스트의 감정이 긍정인지 부정인지 예측하는 모델 생성\n",
    "  - 머신러닝 모델: 선형회귀모델, 랜덤포레스트모델 / 사이킷런 사용\n",
    "  - 딥러닝 모델: 합성곱 신경망(CNN) 모델, 순환 신경망(RNN) 모델 / 텐서플로 사용\n",
    "  \n",
    "#### ① 회귀 모델\n",
    "- 로지스틱 회귀 모델은 주로 이항 분류에 사용\n",
    "- 선형 결합을 통해 나온 결과를 토대로 예측  \n",
    "\n",
    "\n",
    "- **선형회귀모델**\n",
    "  - 종속변수와 독립변수 간 상관관계를 모델링하는 방법\n",
    "  - y = $w_{1}x_{1} + w_{2}x_{2} + ... + b$\n",
    "    - w, b: 학습하고자 하는 파라미터\n",
    "    - x: 입력값 (단어, 문장 표현 벡터)  \n",
    "\n",
    "\n",
    "- **로지스틱 회귀 모델**\n",
    "  - 선형 모델의 결괏값에 로지스틱 함수를 적용해 0-1 사이 값을 갖게 하여 확률로 표현\n",
    "  - 결과가 1에 가까우면 정답이 1이라고 예측하고 0에 가까우면 0으로 예측함\n",
    "  - 로지스틱 모델로 하는 텍스트 분류: 입력값인 단어를 단어 임베딩 벡터로 만들기\n",
    "    - word2vec, tf-idf 등\n",
    "\n",
    "#### ② 회귀모델 1: TF-IDF를 활용한 모델 구현\n",
    "- 사이킷런의 TfidVectorizer 사용 (입력값이 텍스트로 이뤄진 데이터 형태여야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라이브러리 불러오기\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF 벡터화\n",
    "  - min_df: 특정 토큰 df 값이 설정한 값보다 적게 나오면 벡터화 과정에서 제거\n",
    "  - analyzer: 분석하기 위한 기준 단위\n",
    "    - word: 단어 하나 단위\n",
    "    - char: 문자 하나 단위\n",
    "  - sublinear_tf: 문서 단어 빈도 수(term frequency)에 대한 스무딩(smoothing) 여부를 설정하는 값\n",
    "  - ngram_range: 빈도 기본 단위를 어느 범위의 n-gram으로 설정할 것인지 보는 인자\n",
    "  - max_features: 각 벡터의 최대 길이, 특징의 길이를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TfidVectorizer 생성 후, fit transform 함수로 전체 문장에 대한 특징 벡터 데이터 X 생성\n",
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"char\", sublinear_tf=True, ngram_range=(1,3), max_features=5000) \n",
    "\n",
    "X = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 / 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2 #학습 데이터의 20%\n",
    "\n",
    "y = np.array(sentiments) #정답 라벨을 넘파이 배열로 만듦\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 선언 및 학습\n",
    "  - LogisticRegression 클래스 객체 생성\n",
    "  - 객체 생성 후, fit 함수 호출하면 데이터에 대한 모델 학습이 진행됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgs = LogisticRegression(class_weight='balanced') \n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검증 데이터로 성능 평가 (교재에서는 정확도만 측정)\n",
    "  - 학습한 객체의 score 함수 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 검증 데이터로 성능 측정\n",
    "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전처리한 텍스트 형태의 평가 데이터 불러오기\n",
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습 데이터에 사용한 객체를 사용해 TF-IDF 값으로 벡터화\n",
    "## 벡터화할 때, 평가 데이터에 대해서는 fit을 호출하지 않고 그대로 transform만 호출\"\n",
    "testDataVecs = vectorizer.transform(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 예측값을 하나의 변수로 할당하고 출력하여 형태 확인하기\n",
    "test_predicted = lgs.predict(testDataVecs)\n",
    "print(test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './output/word2vec-nlp-tutorial/'\n",
    "\n",
    "answer_dataset = pd.DataFrame({'id': test_data['id'], 'sentiment': test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_tfidf_answer.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ③ 회귀모델 2: word2vec을 활용한 모델 구현\n",
    "- 각 단어를 word2vec으로 벡터화하기\n",
    "- word2vec은 단어로 표현된 리스트를 입력값으로 넣어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 불러오고 각 단어릐 리스트로 나누기\n",
    "reviews = list(train_data['review'])\n",
    "sentiments = list(train_data['sentiment'])\n",
    "\n",
    "## split 함수를 이용해 띄어쓰기 기준으로 리뷰 구분 후 리스트에 추가하여 입력값 생성\n",
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec 벡터화\n",
    "  - word2vec 모델이 하이퍼파라미터 설정\n",
    "    - num_features: 각 단어에 대해 임베딩된 벡터 차원 결정\n",
    "    - min_word_count: 적은 빈도 수의 단어 학습하지 않게 함\n",
    "    - num_workers: 학습을 위한 프로세스 개수 지정\n",
    "    - context: word2vec 수행 위한 컨텍스트 윈도 크기 지정\n",
    "    - downsampling: 빠른 학습을 위해 정답 단어 라벨의 다운 샘플링 비율 지정(보통 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300    \n",
    "min_word_count = 40   \n",
    "num_workers = 4       \n",
    "context = 10          \n",
    "downsampling = 1e-3 # =0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec 학습 진행 상황 확인하는 방법\n",
    "  - 아래와 같이 값을 맞추고 로그 메시지를 INFO로 맞추면 학습 과정에서 INFO 수준으로 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "   level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec 학습 시작\n",
    "  - word2vec 객체를 생성하여 실행\n",
    "  - 학습 후 생성된 객체는 model 변수에 할당됨\n",
    "  - 학습을 위한 객체 인자에는 입력 데이터와 하이퍼파라미터를 순서대로 입력해야 원하는 하이퍼파라미터로 학습할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                          workers=num_workers,\n",
    "                          size=num_features, \n",
    "                          min_count = min_word_count,\n",
    "                          window = context, \n",
    "                          sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word2vec으로 학습한 모델 저장하기\n",
    "## 모델을 저장하면 Word2Vec.load()를 통해 다시 사용할 수 있음\n",
    "DATA_IN_MODEL = './model/'\n",
    "model_name = DATA_IN_MODEL + \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec 모델로 선형회귀모델 학습하기\n",
    "  - 학습을 위해 하나의 리뷰를 같은 형태의 입력값으로 만들어야 함\n",
    "  - 현재 상황\n",
    "    - word2vec 모델에서 각 단어는 벡터로 표현되어 있음\n",
    "    - 리뷰마다 단어의 개수가 달라 입력값을 하나의 형태로 만들어야 함\n",
    "    - 가장 단순한 방법: 문장 모든 단어 벡터값을 평균 내어, 리뷰 하나당 하나의 벡터로 만드는 방법\n",
    "  \n",
    "  \n",
    "- 사용 인자\n",
    "  - words: 단어 모음인 하나의 리뷰가 들어감\n",
    "  - model: word2vec 모델을 넣는 곳, 학습한 word2vec 모델이 들어감\n",
    "  - num_features: word2vec으로 임베딩할 때 정한 벡터의 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector = np.zeros((num_features),dtype=np.float32)\n",
    "    # 속도를 빠르게 하기 위해 미리 0 값을 가지는 벡터를 만듦\n",
    "\n",
    "    num_words = 0\n",
    "    # 어휘사전 준비\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words += 1\n",
    "            # 사전에 해당하는 단어에 단어 벡터를 더함\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "    \n",
    "    # 문장 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 리뷰의 평균 벡터를 구하는 함수 정의\n",
    "  - reviews: 학습 데이터인 전체 리뷰 데이터를 입력하는 인자\n",
    "  - model: word2vec 모델을 입력하는 인자, 앞에서 학습한 모델을 넣음\n",
    "  - num_features:word2vec으로 임베딩할 때 정한 벡터의 차원 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "\n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "\n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전체 리뷰 데이터로 실제 학습에 사용될 입력값 만들기\n",
    "test_data_vecs = get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 / 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_data_vecs\n",
    "y = np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "TEST_SPLIT = 0.2 #학습 데이터의 20%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 선언 및 학습\n",
    "  - TF-IDF 벡터와 동일하게 로지스틱 모델을 사용\n",
    "  - 입력값을 뽑은 특징만 다르고 이외는 모두 동일\n",
    "  - 모델 생성 시, class_weight 인자값을 balanced로 설정하여 각 레벨에 대해 균형 있는 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검증 데이터셋을 이용한 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %f\" % lgs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 제출\n",
    "  - 전처리한 평가 데이터를 불러온 후 리뷰 값을 리스트로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "test_review = list(test_data['review'])\n",
    "\n",
    "print(test_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평가 데이터도 학습 데이터처럼 각 리뷰가 하나의 문자열로 이뤄져 있음  \n",
    "  → 각 단어의 리스트로 만들어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = list()\n",
    "for review in test_review:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2vec으로 임베딩된 벡터값을 할당해야 함\n",
    "  - 단, 평가 데이터에 대해 새롭게 word2vec 모델을 학습시키지 않고 이전에 학습한 모델을 사용해 각 단어를 벡터로 만들어 각 리뷰에 대한 특징값을 만들어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_vecs = get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './output/word2vec-nlp-tutorial'\n",
    "\n",
    "test_predicted = lgs.predict(test_data_vecs)\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "ids = list(test_data['id'])\n",
    "answer_dataset = pd.DataFrame({'id': ids, 'sentiment': test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_w2v_answer.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ④ 랜덤 포레스트 분류 모델\n",
    "- 랜덤 포레스트 모델\n",
    "  - 여러 개의 의사결정 트리의 결괏값을 평균낸 것을 결과로 사용\n",
    "  - 랜덤 포레스트를 통해 분류 혹은 회귀를 수행할 수 있음\n",
    "  \n",
    "\n",
    "- CountVectorizer를 활용한 벡터화\n",
    "  - 모델 구현에 앞서 모델에 사용할 입력값을 정해야 함\n",
    "  - 랜덤 포레스트 모델에서는 CountVectorizer를 사용해 모델의 입력값을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## 분석 단위를 단어로 지정, 각 벡터 최대 길이를 5000으로 설정\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", max_features = 5000) \n",
    "train_data_features = vectorizer.fit_transform(reviews)\n",
    "\n",
    "train_data_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 / 검증 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train_input, eval_input, train_label, eval_label = train_test_split(\n",
    "    train_data_features, y,test_size=TEST_SIZE, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 구현 및 학습\n",
    "  - 랜덤 포레스트는 사이킷런 라이브러리의 RandomForestClassifier 객체로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 랜덤 포레스트 분류기에 의사결정트리 100개 사용\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# 단어 묶음을 벡터화한 데이터와 정답 데이터로 학습 시작\n",
    "forest.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검증 데이터셋으로 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %f\" % forest.score(eval_input, eval_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 제출\n",
    "  - 전처리한 평가 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "\n",
    "test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)\n",
    "\n",
    "test_reviews = list(test_data['review'])\n",
    "ids = list(test_data['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 데이터 백터화\n",
    "test_data_features = vectorizer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 만든 랜덤 포레스트 분류기를 통해 예측값을 가져오기\n",
    "result = forest.predict(test_data_features)\n",
    "\n",
    "# 판다스 데이터 프레임을 통해 데이터를 구성해서 output에 넣기\n",
    "output = pd.DataFrame( data={\"id\": ids, \"sentiment\": result} )\n",
    "\n",
    "# csv파일로 만들기\n",
    "output.to_csv( DATA_OUT_PATH + \"Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⑤ 순환 신경망 분류 모델\n",
    "- 딥러닝을 활용해 분류하는 모델 살펴보기\n",
    "- 순환 신경망(RNN; Recurrent Neural Network)\n",
    "  - 이전 정보(은닉 상태, hidden state)가 점층적으로 쌓이며 현재 정보(입력 상태, input state)를 표현할 수 있는 모델\n",
    "  - 시간에 의존적이거나 순차적인 데이터에 관한 문제에 활용\n",
    "  - 한 단어에 관한 정보를 입력하면 그 다음 나올 단어를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 시각화 함수\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실습 내용\n",
    "  - 영화 평점 예측\n",
    "    - 입력 문장을 순차적으로 입력만 하고 마지막으로 입력한 시점에 출력 정보를 뽑아 영화 평점 예측\n",
    "    - 매 시간 스탭에 따라 입력되는 정보는 은닉 상태를 통해 정보를 다음 시간 스탭으로 전달하게 함\n",
    "    - 마지막 시간 스탭에 나온 은닉 상태는 문장 전체 정보가 담긴 정보로, 이 정보를 활용해 영화 평점을 예측할 수 있도록 로지스틱 회귀 또는 이진 분류를 함\n",
    "    \n",
    "\n",
    "- 랜덤 시드 고정\n",
    "  - 모델과 샘플링을 하는 모든 랜덤 변수 상태를 고정하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 경로 정의\n",
    "DATA_IN_PATH = './data/word2vec-nlp-tutorial/'\n",
    "DATA_OUT_PATH = './output/word2vec-nlp-tutorial/'\n",
    "TRAIN_INPUT_DATA = 'train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)\n",
    "\n",
    "# 파일 로드\n",
    "# 입력 텍스트와 데이터 라벨이 numpy.Array 형식으로 저장되어 np.load로 불러올 수 있음\n",
    "# 입력 텍스트 데이터는 train_input, 데이터 라벨은 train_lael에 할당됨\n",
    "\n",
    "# train_input 데이터에 대해서는 모델 학습 시 텍스트 길이를 맞추기 위해 pad_sequences 함수 사용\n",
    "train_input = np.load(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'rb'))\n",
    "train_input = pad_sequences(train_input, maclen=train_input.shape[1])\n",
    "\n",
    "train_label = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'rb'))\n",
    "\n",
    "# 데이터 사전정보 불러오기 (이전에 데이터 전처리 후 json 파일에 저장)\n",
    "# 모델에 있는 단어 임베딩 크기 정의 시 활용\n",
    "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 하이퍼파라미터 정의\n",
    "  - 모델 학습을 위한 설정: 배치 크기나 에폭 수, 텍스트 데이터 길이, validation 데이터셋 구성 비율 등은 상수로 정의\n",
    "  - 모델 레이어의 차원 수 설정: 모델의 __init__ 함수 파라미터에 입력하기 위해 dict 객체에서 정의, 레이어 차원 수나 드롭아웃 값을 정하는 하이퍼파라미터 명칭은 key에, key에 해당하는 하이퍼파라미터 명칭에 대한 값은 value에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn_classifier_en'\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 5\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "\n",
    "kargs = {'model_name': model_name,\n",
    "        'vocab_size': prepro_configs['vocab_size'],\n",
    "        'embedding_dimension': 100,\n",
    "        'dropout_rate': 0.2,\n",
    "        'lstm_dimension': 150,\n",
    "        'dense_dimension': 150,\n",
    "        'output_dimension':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 구현\n",
    "  - tensorflow.keras 기반으로 구현하고 클래스로 모델을 정의해서 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스로 모델을 구현하기 위해서는 tf.keras.model 상속 필요\n",
    "class RNNClassifier(tf.keras.Model):\n",
    "\n",
    "    # 가장 먼저 구현할 함수는 __init__: RNNClassifier 모델 객체 생성 시마다 실행\n",
    "    # 매개변수로 모델 레이어의 입력 및 출력 차원 수를 정의하는 하이퍼파라미터 정보를 dict 객체로 받음\n",
    "    def __init__(self, **kargs):\n",
    "        \n",
    "        # tf.keras.Model 클래스를 상속받는 경우, super 함수를 통해 부모 클래스에 있는 __init__ 함수 호출\n",
    "        # super 함수로 부모 클래스에 __init__ 함수 인자 모델 이름을 전달하면\n",
    "        # tf.keras.Model을 상속받은 모든 자식은 해당 모델 이름을 공통으로 사용\n",
    "        super(RNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "        \n",
    "        # 텍스트 임베딩 벡터를 위해 layers.Embedding 객체 생성\n",
    "        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'],\n",
    "                                     output_dim=kargs['embedding_dimension'])\n",
    "        \n",
    "        # RNNClassifier 클래스에서는 RNN 계열 모델인 LSTM을 2개 레이어로 활용\n",
    "          # return_sequences: 레이어 출력 차원 수와 출력 시퀀스를 전부 출력할지 여부 판단\n",
    "          # Ture: 시퀀스 형태의 은닉 상태 벡터 출력\n",
    "        # 첫 레이어에서 시퀀스 은닉 상태 벡터를 출력해서 다음 레이어에 입력할 시퀀스 벡터를 구성하고\n",
    "        # 마지막 레이어에서는 시퀀스의 마지막 스탭 은닉 상태 벡터 출력이 필요\n",
    "        # → 첫 번째 레이어에만 True 지정\n",
    "        self.lstm_1_layer = tf.keras.layers.LSTM(kargs['lstm_dimension'], return_sequences=True)\n",
    "        self.lstm_2_layer = tf.keras.layers.LSTM(kargs['lstm_dimension'])\n",
    "        \n",
    "        # 모델 과적합 방지를 위한 레이어 선언\n",
    "        self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
    "        \n",
    "        # RNN 레이어에서 출력한 상태 벡터를 피드 포워드 네트워크를 거치게 함\n",
    "        # 객체 생성 시, 입력 파라미터로 네트워크 출력 시 나오는 벡터 차원 수 units, 네트워크에서 사용할 활성화 함수 지정\n",
    "        # 피드 포워드 네터워크에서는 tanh 함수를 사용하고 tf.keras.activations.tanh 함수를 activation 파라미터에 입력\n",
    "        self.fc1 = layers.Dense(units=kargs['dense_dimension'],\n",
    "                           activation=tf.keras.activations.tanh)\n",
    "        \n",
    "        # 회귀(regression) → Dense 레이어를 통해 예측한 값을 0~1의 값으로 표현할 수 있음\n",
    "        self.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
    "                           activation=tf.keras.activations.sigmoid)\n",
    "    \n",
    "    # call 함수로 __init__을 통해 생성한 레이어 실행 가능\n",
    "    # 입력한 워드 인덱스 시퀀스를 가지고 생성한 네트워트 모듈을 거쳐 마지막에 예측값을 출력\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lstm_1_layer(x)\n",
    "        x = self.lstm_2_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RNNClassifier(**kargs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=1)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\n",
    "\n",
    "checkpoint_path = DATA_OUT_PATH + model_name + '/weights.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 성능 향상을 위해 활용하는 클래스\n",
    "  - tensorflow.keras.callback 모듈의 EarlyStopping과 ModelCheckpoint 클래스 활용\n",
    "  - ModelCheckpoint: 에폭마다 모델을 저장하게 함\n",
    "    - save_best_only: 가장 성능 좋은 모델만 저장\n",
    "    - save_weights_only: 모델 가중치만 저장 (모델 그래프 전부 저장하지 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20분 가량 소요\n",
    "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 성능 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "\n",
    "\n",
    "test_input = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))\n",
    "test_input = pad_sequences(test_input, maxlen=test_input.shape[1])\n",
    "\n",
    "# 베스트 모델 불러오기\n",
    "SAVE_FILE_NM = 'weights.h5'\n",
    "model.load_weights(os.path.join(DATA_OUT_PATH, model_name, SAVE_FILE_NM))\n",
    "\n",
    "# 테스트 데이터 예측하기\n",
    "predictions = model.predict(test_input, batch_size=BATCH_SIZE)\n",
    "predictions = predictions.squeeze(-1)\n",
    "\n",
    "test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'), allow_pickle=True)\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\": list(test_id), \"sentiment\":list(predictions)})\n",
    "output.to_csv(DATA_OUT_PATH + 'movie_review_result_rnn.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⑥ 컨볼루션 신경망 분류 모델\n",
    "- 합성곱 신경망(CNN; Convolutional neural network)\n",
    "  - 전통적인 신경망 앞에 여러 계층의 합성곱(convolution) 계층을 쌓은 모델\n",
    "  - 입력받은 이미지에 대한 가장 좋은 특징을 만들도록 학습하고, 추출된 특징으로 이미지를 분류하는 방식\n",
    "  \n",
    "\n",
    "- RNN이 단어 입력 순서를 중요하게 반영한다면, CNN은 문장 지역 정보를 보존하며, 각 문장 성분 등장 정보를 학습에 반영하는 구조\n",
    "  - 기존의 n-gram 방식과 유사\n",
    "\n",
    "\n",
    "- 모델 구현\n",
    "  - 기본 코드는 RNN 구조와 동일하며, 모델 쪽 코드만 변경하면 CNN 적용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cnn_classifier_en'\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 2\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "\n",
    "kargs = {'model_name': model_name,\n",
    "        'vocab_size': prepro_configs['vocab_size'],\n",
    "        'embedding_size': 128,\n",
    "        'num_filters': 100,\n",
    "        'dropout_rate': 0.5,\n",
    "        'hidden_dimension': 250,\n",
    "        'output_dimension':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 구현\n",
    "  - 텐서플로의 케라스 기반으로 구현하고 클래스 형식으로 모델을 정의해 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'],\n",
    "                                     output_dim=kargs['embedding_size'])\n",
    "        self.conv_list = [layers.Conv1D(filters=kargs['num_filters'],\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   padding='valid',\n",
    "                                   activation=tf.keras.activations.relu,\n",
    "                                   kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "                     for kernel_size in [3,4,5]]\n",
    "        self.pooling = layers.GlobalMaxPooling1D()\n",
    "        self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
    "        self.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
    "                           activation=tf.keras.activations.relu,\n",
    "                           kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "        self.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
    "                           activation=tf.keras.activations.sigmoid,\n",
    "                           kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 성능 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 불러오기\n",
    "DATA_OUT_PATH = './data_out/'\n",
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "\n",
    "test_input = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))\n",
    "test_input = pad_sequences(test_input, maxlen=test_input.shape[1])\n",
    "\n",
    "# 베스트 모델 불러오기\n",
    "SAVE_FILE_NM = 'weights.h5'\n",
    "model.load_weights(os.path.join(DATA_OUT_PATH, model_name, SAVE_FILE_NM))\n",
    "\n",
    "# 테스트 데이터 예측하기\n",
    "predictions = model.predict(test_input, batch_size=BATCH_SIZE)\n",
    "predictions = predictions.squeeze(-1)\n",
    "\n",
    "test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'), allow_pickle=True)\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\": list(test_id), \"sentiment\": list(predictions)} )\n",
    "output.to_csv(DATA_OUT_PATH + 'movie_review_result_cnn.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 성능 결과 정리\n",
    "\n",
    "모델|캐글 점수\n",
    "---|---\n",
    "로지스틱 회귀 모델|0.87400\n",
    "랜덤 포레스트|0.84544\n",
    "순환 신경망(RNN)|0.93818\n",
    "컨볼루션 신경망(CNN)|0.93610"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 한글 텍스트 분류\n",
    "\n",
    "- KoNLPy: 한글 텍스트 분류에 사용하는 라이브러리\n",
    "\n",
    "### 1) 문제 소개\n",
    "- 네이버 영화 리뷰 데이터 \"Naver sentiment movie corpus v1.0\"\n",
    "  - 네이버 영화 사용자 리뷰를 영화당 100개씩 모아서 만든 데이터\n",
    "  - 감정의 경우 긍정 혹은 부정 값을 가짐\n",
    "  \n",
    "\n",
    "- 데이터 전처리 및 분석\n",
    "  - [데이터 다운로드](https://github.com/e9t/nsmc)\n",
    "    - ratings.txt: 전체 리뷰(20만 개 데이터)\n",
    "    - ratings_train.txt: 학습 데이터(15만 개 데이터)\n",
    "    - ratings_test.txt: 평가 데이터(5만 개 데이터)\n",
    "    \n",
    "\n",
    "### 2) 데이터 불러오기 및 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data/naver-movie-review/'\n",
    "\n",
    "print(\"파일 크기 : \")\n",
    "for file in os.listdir(DATA_IN_PATH):\n",
    "    if 'txt' in file :\n",
    "        print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(DATA_IN_PATH + 'ratings_train.txt', header = 0, delimiter = '\\t', quoting = 3)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('전체 학습데이터의 개수: {}'.format(len(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 데이터의 리뷰 길이 확인\n",
    "  - apply 함수로 길이 값 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length = train_data['document'].astype(str).apply(len)\n",
    "train_length.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 변수를 사용하여 전체 데이터의 길이에 대한 히스토그램 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(12, 5))\n",
    "# 히스토그램 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위\n",
    "# range: x축 값의 범위\n",
    "# alpha: 그래프 색상 투명도\n",
    "# color: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(train_length, bins=200, alpha=0.5, color= 'r', label='word')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "# 그래프 제목\n",
    "plt.title('Log-Histogram of length of review')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Length of review')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Number of review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확인\n",
    "  - 140자 제한(한글 기준) 데이터이기 때문에 최대 글자수에 데이터가 모인 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('리뷰 길이 최댓값: {}'.format(np.max(train_length)))\n",
    "print('리뷰 길이 최솟값: {}'.format(np.min(train_length)))\n",
    "print('리뷰 길이 평균값: {:.2f}'.format(np.mean(train_length)))\n",
    "print('리뷰 길이 표준편차: {:.2f}'.format(np.std(train_length)))\n",
    "print('리뷰 길이 중간값: {}'.format(np.median(train_length)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('리뷰 길이 제1사분위: {}'.format(np.percentile(train_length, 25)))\n",
    "print('리뷰 길이 제3사분위: {}'.format(np.percentile(train_length, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 마크함\n",
    "\n",
    "plt.boxplot(train_length,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 어휘 빈도 분석\n",
    "  - 워드클라우드 사용\n",
    "  - 오류 방지를 위해, 문자열이 아닌 데이터는 모두 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review = [review for review in train_data['document'] if type(review) is str]\n",
    "wordcloud = WordCloud(font_path = DATA_IN_PATH + 'NanumSquare.ttf').generate(' '.join(train_review))\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 긍정, 부정을 나타내는 라벨값 비율 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(6, 3)\n",
    "sns.countplot(train_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"긍정 리뷰 개수: {}\".format(train_data['label'].value_counts()[1]))\n",
    "print(\"부정 리뷰 개수: {}\".format(train_data['label'].value_counts()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 개수 히스토그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_counts = train_data['document'].astype(str).apply(lambda x:len(x.split(' ')))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(train_word_counts, bins=50, facecolor='r',label='train')\n",
    "plt.title('Log-Histogram of word count in review', fontsize=15)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Number of reviews', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('리뷰 단어 개수 최댓값: {}'.format(np.max(train_word_counts)))\n",
    "print('리뷰 단어 개수 최솟값: {}'.format(np.min(train_word_counts)))\n",
    "print('리뷰 단어 개수 평균값: {:.2f}'.format(np.mean(train_word_counts)))\n",
    "print('리뷰 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))\n",
    "print('리뷰 단어 개수 중간값: {}'.format(np.median(train_word_counts)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('리뷰 단어 개수 제1사분위: {}'.format(np.percentile(train_word_counts, 25)))\n",
    "print('리뷰 단어 개수 제3사분위: {}'.format(np.percentile(train_word_counts, 75)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특수문자 유무 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmarks = np.mean(train_data['document'].astype(str).apply(lambda x: '?' in x)) # 물음표가 구두점으로 쓰임\n",
    "fullstop = np.mean(train_data['document'].astype(str).apply(lambda x: '.' in x)) # 마침표\n",
    "                  \n",
    "print('물음표가 있는 질문: {:.2f}%'.format(qmarks * 100))\n",
    "print('마침표가 있는 질문: {:.2f}%'.format(fullstop * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판다스의 데이터프레임 형태로 불러오기\n",
    "DATA_IN_PATH ='./data/naver-movie-review/'\n",
    "\n",
    "train_data = pd.read_csv(DATA_IN_PATH + 'ratings_train.txt', header=0, delimiter='\\t', quoting=3 )\n",
    "\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식을 이용해 한글 문자 제외 후 모두 제거\n",
    "review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", train_data['document'][0]) \n",
    "print(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 불용어 제거\n",
    "# 형태소 분석기를 사용할 때, 어간 추출을 적용해 어간이 추출된 단어로 나누기\n",
    "okt=Okt()\n",
    "review_text = okt.morphs(review_text, stem=True)\n",
    "print(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 불용어 사전 만들기\n",
    "# 한글 불용어는 따로 정해져 있지 않음\n",
    "stop_words = set(['은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한'])\n",
    "clean_review = [token for token in review_text if not token in stop_words]\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모든 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 과정을 하나의 함수로 만들기\n",
    "\n",
    "def preprocessing(review, okt, remove_stopwords = False, stop_words = []):\n",
    "    # review: 전처리할 텍스트\n",
    "    # okt: okt 객체를 반복적으로 생성하지 않고 미리 생성후 인자로 받는다.\n",
    "    # remove_stopword: 불용어를 제거할지 선택 기본값은 False\n",
    "    # stop_word: 불용어 사전은 사용자가 직접 입력해야함 기본값은 비어있는 리스트\n",
    "    \n",
    "    # 1. 한글 및 공백을 제외한 문자 모두 제거\n",
    "    review_text = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", review)\n",
    "    \n",
    "    # 2. okt 객체를 활용해서 형태소 단위로 나눔\n",
    "    word_review = okt.morphs(review_text, stem=True)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        \n",
    "        # 불용어 제거(선택적)\n",
    "        word_review = [token for token in word_review if not token in stop_words]\n",
    "        \n",
    "    return word_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 데이터 전처리 진행\n",
    "  - okt 객체 생성, 불용어 사전 정의, 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [ '은', '는', '이', '가', '하', '아', '것', '들','의', '있', '되', '수', '보', '주', '등', '한']\n",
    "okt = Okt()\n",
    "clean_train_review = []\n",
    "\n",
    "for review in train_data ['document']:\n",
    "    if type(review) == str:\n",
    "        clean_train_review.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
    "    else:\n",
    "        clean_train_review.append([])\n",
    "\n",
    "clean_train_review[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평가 데이터에도 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(DATA_IN_PATH + 'ratings_test.txt', header=0, delimiter='\\t', quoting=3 )\n",
    "\n",
    "clean_test_review = []\n",
    "\n",
    "for review in test_data['document']:\n",
    "    # 비어있는 데이터에서 멈추지 않도록 string인 경우만 진행\n",
    "    if type(review) == str:\n",
    "        clean_test_review.append(preprocessing(review, okt, remove_stopwords = True, stop_words=stop_words))\n",
    "    else:\n",
    "        clean_test_review.append([])  #string이 아니면 비어있는 값 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 / 평가 데이터를 인덱스 벡터로 바꾸고 패딩 처ㅣ하기\n",
    "  - 텐서플로의 전처리 모듈 사용\n",
    "    - 토크나이징 객체 생성 → 학습 데이터에만 적용 → 해당 객체로 두 데이터를 인덱스 벡터로 만듦 → 해당 데이터를 패딩 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_train_review)\n",
    "train_sequences = tokenizer.texts_to_sequences(clean_train_review)\n",
    "test_sequences = tokenizer.texts_to_sequences(clean_test_review)\n",
    "\n",
    "word_vocab = tokenizer.word_index # 단어 사전 형태\n",
    "word_vocab[\"<PAD>\"] = 0\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 8 # 문장 최대 길이\n",
    "\n",
    "train_inputs = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') # 학습 데이터를 벡터화\n",
    "train_labels = np.array(train_data['label']) # 학습 데이터의 라벨\n",
    "\n",
    "test_inputs = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') # 테스트 데이터를 벡터화\n",
    "test_labels = np.array(test_data['label']) # 테스트 데이터의 라벨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만든 데이터를 모델링 과정에서 사용할 수 있도록 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_INPUT_DATA = 'nsmc_train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'nsmc_train_label.npy'\n",
    "TEST_INPUT_DATA = 'nsmc_test_input.npy'\n",
    "TEST_LABEL_DATA = 'nsmc_test_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "data_configs = {}\n",
    "\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab) # vocab size 추가\n",
    "\n",
    "# 전처리 된 학습 데이터를 넘파이 형태로 저장\n",
    "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
    "# 전처리 된 테스트 데이터를 넘파이 형태로 저장\n",
    "np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)\n",
    "np.save(open(DATA_IN_PATH + TEST_LABEL_DATA, 'wb'), test_labels)\n",
    "\n",
    "# 데이터 사전을 json 형태로 저장\n",
    "json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 모델링\n",
    "- 실습할 모델: CNN(합성곱 신경망)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 경로 정의\n",
    "DATA_IN_PATH = './data/naver-movie-review/'\n",
    "DATA_OUT_PATH = './output/naver-movie-review/'\n",
    "INPUT_TRAIN_DATA = 'nsmc_train_input.npy'\n",
    "LABEL_TRAIN_DATA = 'nsmc_train_label.npy'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "SEED_NUM = 1234\n",
    "tf.random.set_seed(SEED_NUM)\n",
    "\n",
    "# 파일 로드\n",
    "train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA, 'rb'))\n",
    "train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA, 'rb'))\n",
    "prepro_configs = json.load(open(DATA_IN_PATH + DATA_CONFIGS, 'r'))\n",
    "\n",
    "# 모델 하이퍼파라미터 정의\n",
    "model_name = 'cnn_classifier_kr'\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "VALID_SPLIT = 0.1\n",
    "MAX_LEN = train_input.shape[1]\n",
    "\n",
    "kargs = {'model_name': model_name,\n",
    "        'vocab_size': prepro_configs['vocab_size'],\n",
    "        'embedding_size': 128,\n",
    "        'num_filters': 100,\n",
    "        'dropout_rate': 0.5,\n",
    "        'hidden_dimension': 250,\n",
    "        'output_dimension':1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, **kargs):\n",
    "        super(CNNClassifier, self).__init__(name=kargs['model_name'])\n",
    "        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'],\n",
    "                                     output_dim=kargs['embedding_size'])\n",
    "        self.conv_list = [layers.Conv1D(filters=kargs['num_filters'],\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   padding='valid',\n",
    "                                   activation=tf.keras.activations.relu,\n",
    "                                   kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "                     for kernel_size in [3,4,5]]\n",
    "        self.pooling = layers.GlobalMaxPooling1D()\n",
    "        self.dropout = layers.Dropout(kargs['dropout_rate'])\n",
    "        self.fc1 = layers.Dense(units=kargs['hidden_dimension'],\n",
    "                           activation=tf.keras.activations.relu,\n",
    "                           kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "        self.fc2 = layers.Dense(units=kargs['output_dimension'],\n",
    "                           activation=tf.keras.activations.sigmoid,\n",
    "                           kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier(**kargs)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overfitting을 막기 위한 ealrystop 추가\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=2)\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
    "\n",
    "checkpoint_path = DATA_OUT_PATH + model_name + '/weights.h5'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create path if exists\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
    "    \n",
    "\n",
    "cp_callback = ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 성능 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './output/naver-movie-review/'\n",
    "INPUT_TEST_DATA = 'nsmc_test_input.npy'\n",
    "LABEL_TEST_DATA = 'nsmc_test_label.npy'\n",
    "SAVE_FILE_NM = 'weights.h5' #저장된 best model 이름\n",
    "\n",
    "test_input = np.load(open(DATA_IN_PATH + INPUT_TEST_DATA, 'rb'))\n",
    "test_input = pad_sequences(test_input, maxlen=test_input.shape[1])\n",
    "test_label_data = np.load(open(DATA_IN_PATH + LABEL_TEST_DATA, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(DATA_OUT_PATH, model_name, SAVE_FILE_NM))\n",
    "model.evaluate(test_input, test_label_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
