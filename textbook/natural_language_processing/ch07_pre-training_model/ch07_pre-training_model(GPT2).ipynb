{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ch07. 사전 학습 모델(GPT2)\n",
    "## 03. GPT\n",
    "\n",
    "### 1) GPT1\n",
    "- GPT(Generative Pre-training)1\n",
    "  - 큰 자연어 처리 데이터를 비지도 학습으로 사전 학습한 후, 학습된 가중치로 풀고자 하는 문제를 미세 조정하는 방법론의 모델\n",
    "\n",
    "\n",
    "- 모델 구조\n",
    "  - 트랜스포머 모델 사용(버트와 동일)\n",
    "  - 단, GPT1에서는 트랜스포머의 디코더 구조만 사용(버트는 인코더 구조만 사용)\n",
    "\n",
    "\n",
    "- 사전 학습\n",
    "  - 버트와 달리 하나의 사전 학습 방식(전통적 언어 모델 방식) 사용\n",
    "  - 앞 단어를 활용해 다음 단어를 예측하는 방식으로 사전 학습 진행\n",
    "  - 별도 라벨이 존재하지 않는 데이터도 학습 가능\n",
    "    - 비지도 학습으로 분류\n",
    "  - 많은 데이터로 모델 가중치를 사전 학습할 수 있음\n",
    "  - 버트와 달리, 실제 문제 대상으로 학습 진행 시에도 언어 모델을 함께 학습\n",
    "  \n",
    "input|label\n",
    "---|---\n",
    "\"< START>\"|\"나는\"\n",
    "\"< START>\",\"나는\"|\"학교에\"\n",
    "\"< START>\",\"나는\",\"학교에\"|\"간다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) GPT2\n",
    "- GPT(Generative Pre-training)2\n",
    "  - OpenAI 제안 , 2018년 발표된 GPT1 모델의 성능을 향상한 모델로서 텍스트 생성에서 특히 좋은 성능을 보임\n",
    "  \n",
    "  \n",
    "- 모델 구조\n",
    "  - 대부분 GPT1과 동일, 트랜스포머의 디코더를 기반으로 하는 모델\n",
    "  - 차이점: 레이어 노멀라이제이션이 각 부분 블록의 입력 쪽으로 이동 (기존에는 각 레이어 직후 레지듀얼 커넥션과 함께 적용)\n",
    "\n",
    "\n",
    "- 학습 데이터 및 모델 크기\n",
    "\n",
    "| |GPT1|GPT2|\n",
    "|---|---|---|\n",
    "|레이어|12개|117만 개|\n",
    "|가중치|48개|1,542만 개|\n",
    "\n",
    "\n",
    "### 3) GPT2를 활용한 한국어 언어 생성 모델\n",
    "\n",
    "- 모델 입력\n",
    "  - BPE(Byte Pair Encoding) 방식을 사용해 텍스트를 나누어 입력값으로 사용\n",
    "  - 글자와 문자 사이 적절한 단위를 찾아 나누는 방식으로 높은 성능을 보임\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라이브러리\n",
    "  - TFGPT2LMHeadModel: 문장 생성, GPT2 언어 모델을 만들기 위해 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 학습에 필요한 모듈\n",
    "  - transformers 모듈 중, TFGPT2LMHeadModel\n",
    "  - gluonnlp의 SentencepieceTokenizer, nlp 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(tf.keras.Model):\n",
    "    def __init__(self, dir_path):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
    "    \n",
    "    # vocabulary에 대한 logit 값만 활용하도록 last_hidden_states 출력\n",
    "    def call(self, inputs):\n",
    "        return self.gpt2(inputs)[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습된 파라미터 불러오기\n",
    "  - GPT2의 경우, hugginface에 모델로 등록되어 있지 않아, 파라미터 다운로드가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................................] 460908853 / 460908853"
     ]
    }
   ],
   "source": [
    "# gpt_ckpt 폴더 만드는 코드\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')\n",
    "\n",
    "with zipfile.ZipFile('gpt_ckpt.zip') as z:\n",
    "    z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 객체 생성 시 모델 리소스 경로를 인자로 전달하면, 학습된 파라미터를 가지는 GPT2 모델이 형성됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_PATH = './gpt_ckpt'\n",
    "gpt_model = GPT2Model(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ① 사전 학습 모델 문장 생성\n",
    "- 사전 학습된 GPT2는 언어 모델을 통해 학습   \n",
    "  → GPT2는 언어 모델이며 텍스트 생성이 가능함\n",
    "\n",
    "- 사전 학습된 모델을 활용해 언어 생성 결과를 확인하고 성능 알아보기\n",
    "  - 텍스트를 모델에 입력할 수 있도록 토크나이저 설정 필요\n",
    "  - SentencepieceTokenizer와 nlb 모듈의 vocab으로 단어 사전과 토크나이저 정의\n",
    "  \n",
    "스페셜 토큰|역할\n",
    "---|---\n",
    "< unk>|모르는 단어에 대한 토큰\n",
    "< pad>|배치 데이터 길이 맞추는 용도\n",
    "< s>|문장 시작을 알림\n",
    "< /s>|문장 종결을 알림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "MAX_LEN = 30\n",
    "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
    "\n",
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
    "                                               mask_token=None,\n",
    "                                               sep_token=None,\n",
    "                                               cls_token=None,\n",
    "                                               unknown_token='<unk>',\n",
    "                                               padding_token='<pad>',\n",
    "                                               bos_token='<s>',\n",
    "                                               eos_token='</s>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성된 토크나이저 객체와 사전 학습된 GPT2 모델을 활용해 문장 생성 결과 확인하기\n",
    "\n",
    "- 샘플링 방식\n",
    "  1. top_k 샘플링 방식\n",
    "     - top_k 순위 안에 해당하는 어휘만 샘플링하여 어휘를 예측하는 방식\n",
    "     - top_k 값이 높을수록 무작위 샘플 방식에 가까워지고 낮을수록 탐욕 방식에 가까워짐\n",
    "     - 각 단어의 확률값을 고려하지 않고 순위만 고려하는 방식으로  \n",
    "       모델에서 예측한 단어의 확률이 한 단어에 크게 몰려 있다면(Narrow Distribution)  \n",
    "       확률 낮은 단어가 선택될 수 있어서 생성 결과가 일관되는 좋은 문장을 생성하지 못할 수 있음\n",
    "  \n",
    "  2. 뉴클러스 샘플링(Nucleus Sampling)\n",
    "      - 확률 값의 경계를 top_p로 정하면 확률이 가장 높은 순으로 후보 단어들의 확률을 더했을 때, top_p가 되는 단어 집합에 대해 샘플링\n",
    "      - 일정 확률 값의 경계를 설정하고 문장을 생성했을 경우, top_k 샘플링보다 안정적으로 개연성 있는 문장을 만들 수 있음\n",
    "\n",
    "   - 참고: [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
    "    _logits = logits.numpy()\n",
    "    top_k = min(top_k, logits.shape[-1])  \n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
    "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
    "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
    "        \n",
    "        _logits[indices_to_remove] = filter_value\n",
    "    return tf.constant([_logits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 코드 설명 \n",
    "  - seed_word: 문장 생성의 시작 단어\n",
    "  - model: 문장 생성 수행\n",
    "  - max_step: 생성 횟수 제한\n",
    "  - greedy: 모델 출력 결과에 대해 유연하게 문장 생성을 해줄 수 있는지 선택하게 함\n",
    "    - Ture: 문장 출력 결과에 대해 가장 확률 높은 단어만 선택\n",
    "    - False: 출력 단어 가운데 확률 또는 순위 높은 단어만 선택해 무작위로 생성\n",
    "  - top_k, top_p: greedy가 Flase인 경우에 활용하는 파라미터\n",
    "    - top_k: 확률 높은 순서대로 k번째까지 높은 단어에 대해 필터링하는 값\n",
    "    - top_p: 일정 확률값 이상인 단어에 대해 필터링하는 값\n",
    "    - 두 값이 모두 0이면 필터링하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
    "\n",
    "    # 문장 생성 시작할 단어를 문장을 의미하는 변수에 할당하고 토크나이즈함\n",
    "    sent = seed_word\n",
    "    toked = tokenizer(sent)\n",
    "    \n",
    "    # 문장 생성할 수 있는 반복문에 들어감\n",
    "    # 토크나이즈된 단어를 인덱스로 변환하고 모델에 입력값으로 넣어 출력값을 받음\n",
    "    # 모델 출력값에 대하여는 문장 마지막 단어만 선택하게 함\n",
    "    for _ in range(max_step):\n",
    "        input_ids = tf.constant([vocab[vocab.bos_token],]  + vocab[toked])[None, :] \n",
    "        outputs = model(input_ids)[:, -1, :]\n",
    "        \n",
    "        # 단어 선택 방법\n",
    "        if greedy:\n",
    "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
    "        else:\n",
    "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
    "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
    "        \n",
    "        # 생성된 텍스트 토큰을 대상으로 문자으이 끝을 알리는 </s> 토큰인지 확인\n",
    "        # 해당 과정을 max_step만큼 반복하면 문장이 생성됨\n",
    "        if gen == '</s>':\n",
    "            break\n",
    "        sent += gen.replace('▁', ' ')\n",
    "        toked = tokenizer(sent)\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT2 같은 생성 모델은 주어진 문장 전제로 최대우도추정(Maximum Likelihood Estimation)을 활용해  \n",
    "  단어 예측 분포에서 가장 확률 높은 단어를 선택하는 탐욕 검색(Greedy Search)로 새로운 단어를 예측할 수 있음\n",
    "  - 단, 다음 단어 예측 시 잘못된 단어를 예측하면 학습 상황과 같이  \n",
    "    다음 단어를 예측할 때, 올바른 단어를 출력할 수 있도록 조정하지 않음  \n",
    "  → 확률 높은 단어만 선택하여, 생성된 문장에 어색한 문구가 나오거나 반복되는 단어가 발생할 수 있음\n",
    "  \n",
    "\n",
    "1. 탐욕(greedy) 방식으로 문장 생성하기\n",
    "   - 항상 확률이 가장 높은 단어만 선택하기 때문에 모델이 학습한 바이어스(bias)에 따라 일관된 문장만 출력하게 되고,  \n",
    "     경우에 따라 반복되는 단어가 출력될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때부터                                                                                                   '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('하루', gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. greedy 파라미터를 False로 지정하고 top_k와 top_p를 설정\n",
    "   - 샘플링 방식을 통해 조금 더 자연스럽고 다양한 문장 확인 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때문에 목욕하면 와!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루대이가 기습을 가해 오는 바람에 런텍이 수적 우위를 장악(<unk><lf><mv><md><mc>)><mr><md><md><mc><md> <mv><md><mc> <mc>  <mc><md> <md><md> <md>  <mc>   <md>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('하루', gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ② 소설 텍스트 데이터 전처리하기\n",
    "- 소설 『운수 좋은 날』"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data/finetune/'\n",
    "TRAIN_DATA_FILE = 'finetune_data.txt'\n",
    "\n",
    "sents = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE, encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for s in sents:\n",
    "    tokens = [vocab[vocab.bos_token],]  + vocab[tokenizer(s)] + [vocab[vocab.eos_token],]\n",
    "    input_data.append(tokens[:-1])\n",
    "    output_data.append(tokens[1:])\n",
    "\n",
    "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "\n",
    "input_data = np.array(input_data, dtype=np.int64)\n",
    "output_data = np.array(output_data, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ③ 소설 텍스트 미세 조정 모델 학습\n",
    "- Seq2Seq와 Transformer 모델 사용\n",
    "\n",
    "\n",
    "- 손실 함수와 정확도 측정 함수\n",
    "  - loss_object: 크로스 엔트로피로 손실값을 측정하기 위한 객체\n",
    "  - train_accuracy: 정확도 측정을 위한 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "# loss 함수: 인자로 정답과 예측한 값을 받아 두 개 값을 비교하여 손실 계산\n",
    "# real 값 중, vacab[vocab.padding_token]인 값 <PAD>는 손실 계산에서 빼기 위한 함수\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# accuracy 함수: loss 함수와 비슷하나, train_accuracy 함수를 통해 정확도를 체크한다는 점이 다름\n",
    "def accuracy_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask    \n",
    "    acc = train_accuracy(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.compile(loss=loss_function,\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=[accuracy_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000002B08BE52BE0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000002B08BE52BE0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - ETA: 0s - loss: 4.2235 - accuracy_function: 0.1782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 88s 6s/step - loss: 4.2235 - accuracy_function: 0.1782 - val_loss: 2.9008 - val_accuracy_function: 0.2147\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 80s 5s/step - loss: 2.9587 - accuracy_function: 0.2385 - val_loss: 2.4976 - val_accuracy_function: 0.2538\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 85s 5s/step - loss: 2.5146 - accuracy_function: 0.2686 - val_loss: 2.3377 - val_accuracy_function: 0.2830\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 87s 5s/step - loss: 2.2447 - accuracy_function: 0.2968 - val_loss: 2.2605 - val_accuracy_function: 0.3059\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 78s 5s/step - loss: 2.0203 - accuracy_function: 0.3173 - val_loss: 2.2218 - val_accuracy_function: 0.3259\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 74s 5s/step - loss: 1.7967 - accuracy_function: 0.3368 - val_loss: 2.2407 - val_accuracy_function: 0.3454\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 73s 5s/step - loss: 1.6394 - accuracy_function: 0.3542 - val_loss: 2.2667 - val_accuracy_function: 0.3618\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 74s 5s/step - loss: 1.4742 - accuracy_function: 0.3706 - val_loss: 2.2917 - val_accuracy_function: 0.3776\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 74s 5s/step - loss: 1.3113 - accuracy_function: 0.3860 - val_loss: 2.4117 - val_accuracy_function: 0.3927\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 74s 5s/step - loss: 1.1498 - accuracy_function: 0.4010 - val_loss: 2.5012 - val_accuracy_function: 0.4079\n"
     ]
    }
   ],
   "source": [
    "history = gpt_model.fit(input_data, output_data, \n",
    "                    batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/finetune/tf2_gpt2_finetuned_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "DATA_OUT_PATH = './output/finetune/'\n",
    "model_name = \"tf2_gpt2_finetuned_model\"\n",
    "\n",
    "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "gpt_model.gpt2.save_pretrained(save_path)\n",
    "\n",
    "loaded_gpt_model = GPT2Model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때                                                                                                    '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루에                                                                                                   '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('하루', gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때,                                                                                                   '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루에게는                                                                                                 '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('하루', gpt_model, top_k=0, top_p=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
